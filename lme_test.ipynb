{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First set the size of the train/test set (they need to be equal)\n",
    "And then run the entire notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "nobs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import edward as ed\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "from edward.models import Normal, BernoulliWithSigmoidProbs, Bernoulli, InverseGamma, Empirical, StudentT\n",
    "from observations import insteval\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import logistic\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "ed.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {0: dict(prob = 0.2, w = -0.5,   # cluster weight in logistic\n",
    "                    sites = np.arange(6),     \n",
    "                    site_probs = [0.1, 0.1, 0.1, 0.1, 0.1, 0.5],\n",
    "                    w_s =        [0.0, 0.0, 0.0, 5.0, 0.0, 0.0]),  # weights of sites in logistic\n",
    "            1: dict(prob = 0.5, w = -1.0,  \n",
    "                    sites = 6 + np.arange(3), \n",
    "                    site_probs = [0.1, 0.3, 0.6],\n",
    "                    w_s        = [0.0, 0.0, 0.0]),                    \n",
    "            2: dict(prob = 0.3, w = -0.2,  \n",
    "                    sites = 9 + np.arange(3), \n",
    "                    site_probs = [0.3, 0.3, 0.4] ,\n",
    "                    w_s =        [0.0, -4, 0.0 ]\n",
    "                   )}\n",
    "\n",
    "\n",
    "# num clusters\n",
    "n_c = len(clusters)\n",
    "\n",
    "# num sites\n",
    "n_s = sum(list( map(len, list( map( lambda d: d['sites'], clusters.values()))))) # num sites\n",
    "\n",
    "# site to cluster map\n",
    "s2c = dict( [ (s,c) for c in range(n_c) for s in clusters[c]['sites']] )\n",
    "\n",
    "# prob of each cluster occurring\n",
    "p_c = [c['prob'] for c in clusters.values()]\n",
    "\n",
    "# prob of site occurrence, within a cluster\n",
    "\n",
    "p_s = ( [  dict( zip (d['sites'], d['site_probs']))  for d in clusters.values() ])\n",
    "\n",
    "# \"true\" weights of clusters in logit model\n",
    "w_c = np.array( [d['w'] for d in clusters.values()] )\n",
    "\n",
    "# \"true\" weights of sites in logit model\n",
    "w_s = np.concatenate( [d['w_s'] for d in clusters.values() ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# logit for a site: sum of site-weight and its cluster-weight\n",
    "def logit_site(site):\n",
    "    return w_c[s2c[site]] + w_s[site]\n",
    "\n",
    "# site response_rate\n",
    "def rr_s(site):\n",
    "    return logistic.cdf(logit_site(site))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_prob_label(site):\n",
    "    p = rr_s(site)\n",
    "    return p, (np.random.uniform() < p)*1\n",
    "\n",
    "\n",
    "def gen_row(cluster):\n",
    "    site2prob = p_s[cluster]\n",
    "    site_ids = list( site2prob.keys())\n",
    "    probs = list(site2prob.values())\n",
    "    site = site_ids[ np.random.choice(len(probs), 1, probs ) [0] ]\n",
    "    prob, label = gen_prob_label(site)\n",
    "    return [ cluster, site, prob, label ] \n",
    "\n",
    "def gen_data(N=100):\n",
    "    clusters = np.random.choice(n_c, N, list(p_c))\n",
    "    data = list(map(gen_row, clusters))\n",
    "    df = pd.DataFrame( data, columns = ['cluster', 'site', 'prob', 'label'])\n",
    "    features = dict(df[['cluster', 'site', 'prob']])\n",
    "    labels = np.array(list( df['label']))\n",
    "    return features, labels\n",
    "\n",
    "def log_loss(labels, probs):\n",
    "    return -np.mean(labels * np.log(probs) + (1-labels)*np.log(1-probs))\n",
    "\n",
    "def rig(labels, probs):\n",
    "    p = np.mean(labels)\n",
    "    ent = -p*np.log(p) - (1-p)*np.log(1-p)\n",
    "    loss = log_loss(labels, probs)\n",
    "    return np.round(100*(ent - loss)/ent, 2)\n",
    "\n",
    "# CAUTION: train, test size need to be same \n",
    "# because we hard-coded n_obs_train in the rand var defns below\n",
    "x_train, y_train = gen_data(N=nobs)\n",
    "x_test, y_test = gen_data(N=nobs)\n",
    "\n",
    "c_train = x_train['cluster']\n",
    "s_train = x_train['site']\n",
    "n_obs_train = len(c_train)\n",
    "\n",
    "c_test = x_test['cluster']\n",
    "s_test = x_test['site']\n",
    "n_obs_test = len(c_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c_train = train[:,0]\n",
    "# s_train = train[:,1]\n",
    "# y_train = train[:,2]\n",
    "# n_obs_train = train.shape[0]\n",
    "\n",
    "# c_test = test[:,0]\n",
    "# s_test = test[:,1]\n",
    "# y_test = np.array(test[:,2]).astype(np.int32)\n",
    "# n_obs_test = test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train + eval with MLE/TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/8t/pw3z265s66x05f3yfpynnr5s3_vhgw/T/tmpj0iukdx_\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/8t/pw3z265s66x05f3yfpynnr5s3_vhgw/T/tmpj0iukdx_', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1c20c51358>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/8t/pw3z265s66x05f3yfpynnr5s3_vhgw/T/tmpj0iukdx_/model.ckpt.\n",
      "INFO:tensorflow:loss = 138.62949, step = 1\n",
      "INFO:tensorflow:global_step/sec: 550.406\n",
      "INFO:tensorflow:loss = 118.86299, step = 101 (0.183 sec)\n",
      "INFO:tensorflow:global_step/sec: 761.613\n",
      "INFO:tensorflow:loss = 120.67326, step = 201 (0.131 sec)\n",
      "INFO:tensorflow:global_step/sec: 679.898\n",
      "INFO:tensorflow:loss = 110.10007, step = 301 (0.147 sec)\n",
      "INFO:tensorflow:global_step/sec: 584.098\n",
      "INFO:tensorflow:loss = 99.775665, step = 401 (0.171 sec)\n",
      "INFO:tensorflow:global_step/sec: 558.141\n",
      "INFO:tensorflow:loss = 112.941376, step = 501 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 560.074\n",
      "INFO:tensorflow:loss = 111.23826, step = 601 (0.178 sec)\n",
      "INFO:tensorflow:global_step/sec: 569.829\n",
      "INFO:tensorflow:loss = 109.95914, step = 701 (0.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 562.238\n",
      "INFO:tensorflow:loss = 123.36773, step = 801 (0.178 sec)\n",
      "INFO:tensorflow:global_step/sec: 554.489\n",
      "INFO:tensorflow:loss = 111.32265, step = 901 (0.180 sec)\n",
      "INFO:tensorflow:global_step/sec: 777.521\n",
      "INFO:tensorflow:loss = 103.48546, step = 1001 (0.128 sec)\n",
      "INFO:tensorflow:global_step/sec: 790.151\n",
      "INFO:tensorflow:loss = 107.52116, step = 1101 (0.127 sec)\n",
      "INFO:tensorflow:global_step/sec: 770.992\n",
      "INFO:tensorflow:loss = 112.450485, step = 1201 (0.130 sec)\n",
      "INFO:tensorflow:global_step/sec: 780.307\n",
      "INFO:tensorflow:loss = 103.48917, step = 1301 (0.128 sec)\n",
      "INFO:tensorflow:global_step/sec: 807.381\n",
      "INFO:tensorflow:loss = 106.524414, step = 1401 (0.124 sec)\n",
      "INFO:tensorflow:global_step/sec: 781.703\n",
      "INFO:tensorflow:loss = 109.141136, step = 1501 (0.128 sec)\n",
      "INFO:tensorflow:global_step/sec: 865.142\n",
      "INFO:tensorflow:loss = 111.871185, step = 1601 (0.115 sec)\n",
      "INFO:tensorflow:global_step/sec: 796.571\n",
      "INFO:tensorflow:loss = 109.91643, step = 1701 (0.126 sec)\n",
      "INFO:tensorflow:global_step/sec: 780.915\n",
      "INFO:tensorflow:loss = 112.61444, step = 1801 (0.128 sec)\n",
      "INFO:tensorflow:global_step/sec: 783.508\n",
      "INFO:tensorflow:loss = 110.38315, step = 1901 (0.128 sec)\n",
      "INFO:tensorflow:global_step/sec: 766.461\n",
      "INFO:tensorflow:loss = 111.55631, step = 2001 (0.131 sec)\n",
      "INFO:tensorflow:global_step/sec: 672.097\n",
      "INFO:tensorflow:loss = 107.19939, step = 2101 (0.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 732.488\n",
      "INFO:tensorflow:loss = 110.659805, step = 2201 (0.136 sec)\n",
      "INFO:tensorflow:global_step/sec: 807.429\n",
      "INFO:tensorflow:loss = 100.82237, step = 2301 (0.124 sec)\n",
      "INFO:tensorflow:global_step/sec: 778.051\n",
      "INFO:tensorflow:loss = 103.08591, step = 2401 (0.129 sec)\n",
      "INFO:tensorflow:global_step/sec: 536.859\n",
      "INFO:tensorflow:loss = 101.67002, step = 2501 (0.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 421.191\n",
      "INFO:tensorflow:loss = 103.06024, step = 2601 (0.238 sec)\n",
      "INFO:tensorflow:global_step/sec: 463.771\n",
      "INFO:tensorflow:loss = 112.90274, step = 2701 (0.214 sec)\n",
      "INFO:tensorflow:global_step/sec: 649.384\n",
      "INFO:tensorflow:loss = 108.56427, step = 2801 (0.154 sec)\n",
      "INFO:tensorflow:global_step/sec: 745.335\n",
      "INFO:tensorflow:loss = 106.04653, step = 2901 (0.134 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3000 into /var/folders/8t/pw3z265s66x05f3yfpynnr5s3_vhgw/T/tmpj0iukdx_/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 99.54289.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-08-15:24:37\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/8t/pw3z265s66x05f3yfpynnr5s3_vhgw/T/tmpj0iukdx_/model.ckpt-3000\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-08-15:24:38\n",
      "INFO:tensorflow:Saving dict for global step 3000: accuracy = 0.696, accuracy_baseline = 0.656, auc = 0.6874756, auc_precision_recall = 0.5709001, average_loss = 0.5655298, global_step = 3000, label/mean = 0.344, loss = 113.10597, prediction/mean = 0.34255952\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/8t/pw3z265s66x05f3yfpynnr5s3_vhgw/T/tmpj0iukdx_/model.ckpt-3000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def input_fn(features, labels,  batch_size = int(nobs/5), test = False):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "    if not test:\n",
    "        dataset = dataset.shuffle(1000).repeat()\n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "\n",
    "feature_cols = [#tf.feature_column.categorical_column_with_identity(key = 'cluster', num_buckets=n_c),\n",
    "                tf.feature_column.categorical_column_with_identity(key = 'site', num_buckets = n_s)\n",
    "               ]\n",
    "\n",
    "classifier = tf.estimator.LinearClassifier(feature_columns=feature_cols, optimizer=tf.train.AdamOptimizer(learning_rate=0.01))\n",
    "\n",
    "classifier.train(input_fn=lambda: input_fn(x_train, y_train), steps=3000)\n",
    "\n",
    "# Evaluate the model.\n",
    "eval_result = classifier.evaluate(input_fn=lambda: input_fn(x_test, y_test, test=True))\n",
    "\n",
    "y_pred = classifier.predict(input_fn=lambda: input_fn(x_test, y_test, test=True))\n",
    "\n",
    "probs = np.array( [yp['probabilities'][1] for yp in y_pred] )  # predicted probabilities for class[1] \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max possible RIG and RIG from MLE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%RIG-MLE =  12.14 %RIG-ideal =  12.79\n"
     ]
    }
   ],
   "source": [
    "\n",
    "RIG = rig(y_test, probs)\n",
    "\n",
    "# true logistic model probabilities \n",
    "true_probs = np.array(list(x_test['prob']))\n",
    "\n",
    "# RIG if we knew the true logistic model\n",
    "ideal_RIG = rig(y_test, true_probs)\n",
    "\n",
    "print(\"%RIG-MLE = \", RIG, \"%RIG-ideal = \", ideal_RIG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Prior distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Set up placeholders for the data inputs.\n",
    "N = tf.placeholder(tf.int32, [])\n",
    "s_ph = tf.placeholder(tf.int32, [None])\n",
    "c_ph = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# Set up random effects.\n",
    "#sigma_c = tf.sqrt(tf.exp(tf.get_variable(\"sigma_c\", [])))\n",
    "eta_c = Normal(loc=tf.zeros(n_c), scale=0.01*tf.ones(n_c))#sigma_c*tf.ones(n_c))\n",
    "\n",
    "w_s = Normal(loc=tf.zeros(n_s), scale=tf.ones(n_s))\n",
    "b = Normal(loc=tf.zeros(1), scale=tf.ones(1))\n",
    "logits_traditional = Normal(loc= b + tf.gather(w_s,s_ph), scale=tf.ones(n_obs_train) )\n",
    "logits_varying_intercept = Normal(loc= b + tf.gather(w_s,s_ph) + tf.gather(eta_c, c_ph), scale=tf.ones(n_obs_train))\n",
    "y_traditional = Bernoulli(logits=logits_traditional)\n",
    "y_varying_intercept = Bernoulli(logits=logits_varying_intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "qw_s_traditional = Normal(loc=tf.get_variable(\"qw_s_trad/loc\", [n_s],initializer=tf.random_normal_initializer),\n",
    "            scale=tf.nn.softplus(tf.get_variable(\"qw_s_trad/scale\", [n_s], initializer=tf.random_normal_initializer)))\n",
    "qb_traditional = Normal(loc=tf.get_variable(\"qb_trad/loc\", [1], initializer=tf.random_normal_initializer),\n",
    "            scale=tf.nn.softplus(tf.get_variable(\"qb_trad/scale\", [1],initializer=tf.random_normal_initializer)))\n",
    "\n",
    "qw_s_varying_intercept = Normal(loc=tf.get_variable(\"qw_s_vi/loc\", [n_s],initializer=tf.random_normal_initializer),\n",
    "            scale=tf.nn.softplus(tf.get_variable(\"qw_s_vi/scale\", [n_s],initializer=tf.random_normal_initializer)))\n",
    "qb_varying_intercept = Normal(loc=tf.get_variable(\"qb_vi/loc\", [1],initializer=tf.random_normal_initializer),\n",
    "            scale=tf.nn.softplus(tf.get_variable(\"qb_vi/scale\", [1],initializer=tf.random_normal_initializer)))\n",
    "q_eta_c = Normal(\n",
    "    loc=tf.get_variable(\"q_eta_c/loc\", [n_c]),\n",
    "    scale=tf.nn.softplus(tf.get_variable(\"q_eta_c/scale\", [n_c],initializer=tf.random_normal_initializer)))\n",
    "\n",
    "\n",
    "\n",
    "latent_vars_traditional = {\n",
    "    b : qb_traditional,\n",
    "    w_s: qw_s_traditional,\n",
    "    eta_c: q_eta_c}\n",
    "\n",
    "latent_vars_varying_intercept = {\n",
    "    b : qb_varying_intercept,\n",
    "    w_s: qw_s_varying_intercept,\n",
    "    eta_c: q_eta_c}\n",
    "\n",
    "data_varying_intercept = {\n",
    "    y_varying_intercept: y_train,\n",
    "    s_ph: s_train,\n",
    "    c_ph: c_train,\n",
    "    N: n_obs_train\n",
    "}\n",
    "\n",
    "data_test_varying_intercept = {\n",
    "    s_ph: s_test,\n",
    "    c_ph: c_test,\n",
    "    N: n_obs_test\n",
    "}\n",
    "\n",
    "data_traditional = {\n",
    "    y_traditional: y_train,\n",
    "    s_ph: s_train,\n",
    "    N: n_obs_train\n",
    "}\n",
    "\n",
    "data_test_traditional = {\n",
    "    s_ph: s_test,\n",
    "    N: n_obs_test\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pchalasani/miniconda/envs/tfbrain/lib/python3.6/site-packages/edward/util/random_variables.py:52: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  not np.issubdtype(value.dtype, np.float) and \\\n",
      "/Users/pchalasani/miniconda/envs/tfbrain/lib/python3.6/site-packages/edward/util/random_variables.py:53: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  not np.issubdtype(value.dtype, np.int) and \\\n"
     ]
    }
   ],
   "source": [
    "inference_traditional = ed.KLqp(latent_vars_traditional, data_traditional)\n",
    "\n",
    "inference_varying_intercept = ed.KLqp(latent_vars_varying_intercept, data_varying_intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_traditional_post = ed.copy(logits_traditional, latent_vars_traditional)\n",
    "logits_vi_post = ed.copy(logits_varying_intercept, latent_vars_varying_intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_traditional = ed.copy(y_traditional, latent_vars_traditional)\n",
    "y_test_varying_intercept = ed.copy(y_varying_intercept, latent_vars_varying_intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5800/10000 [ 57%] █████████████████              ETA: 5s | Loss: 642.927"
     ]
    }
   ],
   "source": [
    "inference_varying_intercept.initialize(n_print=100, n_iter=10000)\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "for i in range(inference_varying_intercept.n_iter):\n",
    "  # Update and print progress of algorithm.\n",
    "  info_dict = inference_varying_intercept.update()\n",
    "  inference_varying_intercept.print_progress(info_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_traditional.initialize(n_print=100, n_iter=10000)\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "for i in range(inference_traditional.n_iter):\n",
    "  # Update and print progress of algorithm.\n",
    "  info_dict = inference_traditional.update()\n",
    "  inference_traditional.print_progress(info_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_param(w, data=None, n_samples=100, title=''):\n",
    "    ax = plt.axes()\n",
    "    if data is None:\n",
    "        plot_vals = w.sample(n_samples).eval().squeeze()\n",
    "    else:\n",
    "        plot_vals = w.sample(n_samples,).eval(data).squeeze()\n",
    "    try:\n",
    "        D = plot_vals.shape[1]\n",
    "        for i  in range(D):\n",
    "            sns.kdeplot(plot_vals[:,i], ax=ax)\n",
    "    except IndexError:\n",
    "        sns.kdeplot(plot_vals,color='green',ax=ax)\n",
    "    ax.set_title(title)\n",
    "\n",
    "def plot_posterior(w, data=None, n_samples=2, probs=False, title=''):\n",
    "    ax = plt.axes()\n",
    "    if probs is True:\n",
    "        plot_vals = expit(w.sample(n_samples).eval(data).squeeze())\n",
    "    else:\n",
    "        plot_vals = w.sample(n_samples,).eval(data).squeeze()\n",
    "    D = plot_vals.shape[0]\n",
    "    for i  in range(D):     \n",
    "        sns.kdeplot(plot_vals[i,:],ax=ax)\n",
    "    ax.set_title(title)\n",
    "    return plot_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional Logistic - Parameter Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_param(qb_traditional, title='Distribution of Intercepts-Logistic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_param(qw_s_traditional, title='Distribution of Slopes-Logistic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=plot_posterior(logits_traditional_post, data_traditional, n_samples=10,probs=True,\n",
    "                 title='Distribution of Posterior-Logistic')\n",
    "plt.vlines(y_train.sum()/n_obs_train,0,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RIG with Traditional Bayesian Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_trad = logits_traditional_post.sample(1000).eval(data_test_traditional).mean(axis=0)\n",
    "probs_trad = logistic.cdf(z_trad)\n",
    "rig_trad = rig(y_test, probs_trad)\n",
    "print(\"%RIG-trad = \", rig_trad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RIG with Varying-intercept Bayesian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_vi = logits_vi_post.sample(100).eval(data_test_varying_intercept).mean(axis=0)\n",
    "probs_vi = logistic.cdf(z_vi)\n",
    "rig_vi = rig(y_test, probs_vi)\n",
    "print(\"%RIG-VI = \", rig_vi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def one_hot(targets, nb_classes):\n",
    "    return np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "\n",
    "lr = LogisticRegression(fit_intercept=True)\n",
    "lr.fit(one_hot(s_train, n_s), y_train)\n",
    "print(lr.coef_)\n",
    "print(lr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_s_traditional= qw_s_traditional.sample(1000).eval().mean(axis=0)\n",
    "b_traditional  = qb_traditional.sample(1000).eval().mean()\n",
    "print(w_s_traditional)\n",
    "print(b_traditional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed Effect - Cluster Varying Intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_param(qb_varying_intercept, title='Distribution of Intercepts-Mixed Effects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_param(qw_s_varying_intercept, title='Distribution of Slope-Mixed Effects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_param(q_eta_c,title='Distribution of Cluster Intercepts -Mixed Effects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=plot_posterior(logits_varying_intercept, data_varying_intercept, n_samples=100, probs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
