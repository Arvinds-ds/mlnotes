{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Linear Mixed Effects Models -- modified Edward tutorial for grouped model\n",
    "\n",
    "With linear mixed effects models, we wish to model a linear\n",
    "relationship for data points with inputs of varying type, categorized\n",
    "into subgroups, and associated to a real-valued output.\n",
    "\n",
    "We demonstrate with an example in Edward. A webpage version is available \n",
    "[here](http://edwardlib.org/tutorials/linear-mixed-effects-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import edward as ed\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from edward.models import Normal, BernoulliWithSigmoidProbs, Bernoulli\n",
    "from observations import insteval\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import logistic\n",
    "\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "ed.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Data: Clusters and site parameters\n",
    "\n",
    "We will define 3 clusters, each with some number of member sites.\n",
    "\n",
    "The data-generating model follows a very simple premise:\n",
    "> Within each cluster, the sites will have a \"default\" or \"baseline\" RR (response-rate), and sites within a cluster will exhibit variation around this cluster-default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# site, cluster occurrence probabilities, to generate the dataset\n",
    "# as well as their \"true\" logit-weights \n",
    "\n",
    "clusters = {0: dict(prob = 0.2, w = -0.5,   # cluster weight in logistic\n",
    "                    sites = np.arange(6),     \n",
    "                    site_probs = [0.1, 0.1, 0.1, 0.1, 0.1, 0.5],\n",
    "                    w_s =        [0.0, 0.0, 0.0, 5.0, 0.0, 0.0]),  # weights of sites in logistic\n",
    "            1: dict(prob = 0.5, w = -1.0,  \n",
    "                    sites = 6 + np.arange(3), \n",
    "                    site_probs = [0.1, 0.3, 0.6],\n",
    "                    w_s        = [0.0, 0.0, 0.0]),                    \n",
    "            2: dict(prob = 0.3, w = -0.2,  \n",
    "                    sites = 9 + np.arange(3), \n",
    "                    site_probs = [0.3, 0.3, 0.4] ,\n",
    "                    w_s =        [0.0, -4, 0.0 ]\n",
    "                   )}\n",
    "\n",
    "\n",
    "# num clusters\n",
    "n_c = len(clusters)\n",
    "\n",
    "# num sites\n",
    "n_s = sum(list( map(len, list( map( lambda d: d['sites'], clusters.values()))))) # num sites\n",
    "\n",
    "# site to cluster map\n",
    "s2c = dict( [ (s,c) for c in range(n_c) for s in clusters[c]['sites']] )\n",
    "\n",
    "# prob of each cluster occurring\n",
    "p_c = [c['prob'] for c in clusters.values()]\n",
    "\n",
    "# prob of site occurrence, within a cluster\n",
    "\n",
    "p_s = ( [  dict( zip (d['sites'], d['site_probs']))  for d in clusters.values() ])\n",
    "\n",
    "# \"true\" weights of clusters in logit model\n",
    "w_c = np.array( [d['w'] for d in clusters.values()] )\n",
    "\n",
    "# \"true\" weights of sites in logit model\n",
    "w_s = np.concatenate( [d['w_s'] for d in clusters.values() ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True weights of logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logit for a site: sum of site-weight and its cluster-weight\n",
    "def logit_site(site):\n",
    "    return w_c[s2c[site]] + w_s[site]\n",
    "\n",
    "# site response_rate\n",
    "def rr_s(site):\n",
    "    return logistic.cdf(logit_site(site))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Site response-rates\n",
    "Note how this matches what we wanted to model, i.e. in each cluster sites have a certain \"baseline\" response rate (RR), and some have much higher or much lower RR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3775,\n",
       " 0.3775,\n",
       " 0.3775,\n",
       " 0.989,\n",
       " 0.3775,\n",
       " 0.3775,\n",
       " 0.2689,\n",
       " 0.2689,\n",
       " 0.2689,\n",
       " 0.4502,\n",
       " 0.0148,\n",
       " 0.4502]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.round(rr_s(s),4) for s in range(n_s) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data gen \n",
    "We generate N rows with `[cluster_id, site_id, abel]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_prob_label(site):\n",
    "    p = rr_s(site)\n",
    "    return p, (np.random.uniform() < p)*1\n",
    "\n",
    "\n",
    "def gen_row(cluster):\n",
    "    site2prob = p_s[cluster]\n",
    "    site_ids = list( site2prob.keys())\n",
    "    probs = list(site2prob.values())\n",
    "    site = site_ids[ np.random.choice(len(probs), 1, probs ) [0] ]\n",
    "    prob, label = gen_prob_label(site)\n",
    "    return [ cluster, site, prob, label ] \n",
    "\n",
    "def gen_data(N=100):\n",
    "    clusters = np.random.choice(n_c, N, list(p_c))\n",
    "    data = list(map(gen_row, clusters))\n",
    "    df = pd.DataFrame( data, columns = ['cluster', 'site', 'prob', 'label'])\n",
    "    features = dict(df[['cluster', 'site', 'prob']])\n",
    "    labels = np.array(list( df['label']))\n",
    "    return features, labels\n",
    "\n",
    "def log_loss(labels, probs):\n",
    "    return -np.mean(labels * np.log(probs) + (1-labels)*np.log(1-probs))\n",
    "\n",
    "def rig(labels, probs):\n",
    "    p = np.mean(labels)\n",
    "    ent = -p*np.log(p) - (1-p)*np.log(1-p)\n",
    "    loss = log_loss(labels, probs)\n",
    "    return np.round(100*(ent - loss)/ent, 2)\n",
    "\n",
    "x_train, y_train = gen_data(N=800)\n",
    "x_test, y_test = gen_data(N=800)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>prob</th>\n",
       "      <th>site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.014774</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.377541</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.450166</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.450166</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.377541</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster      prob  site\n",
       "0        2  0.014774    10\n",
       "1        0  0.377541     5\n",
       "2        2  0.450166    11\n",
       "3        2  0.450166     9\n",
       "4        0  0.377541     4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x_train).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quickly train + eval with MLE/TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/8t/pw3z265s66x05f3yfpynnr5s3_vhgw/T/tmpupocc4kz\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/8t/pw3z265s66x05f3yfpynnr5s3_vhgw/T/tmpupocc4kz', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1c1c3e5198>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/8t/pw3z265s66x05f3yfpynnr5s3_vhgw/T/tmpupocc4kz/model.ckpt.\n",
      "INFO:tensorflow:loss = 69.31474, step = 1\n",
      "INFO:tensorflow:global_step/sec: 505.026\n",
      "INFO:tensorflow:loss = 50.97311, step = 101 (0.199 sec)\n",
      "INFO:tensorflow:global_step/sec: 771.992\n",
      "INFO:tensorflow:loss = 59.639168, step = 201 (0.130 sec)\n",
      "INFO:tensorflow:global_step/sec: 800.877\n",
      "INFO:tensorflow:loss = 56.68206, step = 301 (0.125 sec)\n",
      "INFO:tensorflow:global_step/sec: 753.466\n",
      "INFO:tensorflow:loss = 47.702103, step = 401 (0.133 sec)\n",
      "INFO:tensorflow:global_step/sec: 828.48\n",
      "INFO:tensorflow:loss = 52.088203, step = 501 (0.121 sec)\n",
      "INFO:tensorflow:global_step/sec: 813.895\n",
      "INFO:tensorflow:loss = 49.13678, step = 601 (0.123 sec)\n",
      "INFO:tensorflow:global_step/sec: 836.12\n",
      "INFO:tensorflow:loss = 54.628147, step = 701 (0.120 sec)\n",
      "INFO:tensorflow:global_step/sec: 844.444\n",
      "INFO:tensorflow:loss = 53.24907, step = 801 (0.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 804.086\n",
      "INFO:tensorflow:loss = 51.00796, step = 901 (0.124 sec)\n",
      "INFO:tensorflow:global_step/sec: 808.262\n",
      "INFO:tensorflow:loss = 51.00892, step = 1001 (0.124 sec)\n",
      "INFO:tensorflow:global_step/sec: 792.954\n",
      "INFO:tensorflow:loss = 52.455963, step = 1101 (0.126 sec)\n",
      "INFO:tensorflow:global_step/sec: 743.373\n",
      "INFO:tensorflow:loss = 49.61481, step = 1201 (0.135 sec)\n",
      "INFO:tensorflow:global_step/sec: 770.505\n",
      "INFO:tensorflow:loss = 52.44367, step = 1301 (0.130 sec)\n",
      "INFO:tensorflow:global_step/sec: 800.609\n",
      "INFO:tensorflow:loss = 54.63157, step = 1401 (0.125 sec)\n",
      "INFO:tensorflow:global_step/sec: 835.532\n",
      "INFO:tensorflow:loss = 55.3757, step = 1501 (0.120 sec)\n",
      "INFO:tensorflow:global_step/sec: 821.28\n",
      "INFO:tensorflow:loss = 48.32911, step = 1601 (0.122 sec)\n",
      "INFO:tensorflow:global_step/sec: 847.704\n",
      "INFO:tensorflow:loss = 56.21086, step = 1701 (0.118 sec)\n",
      "INFO:tensorflow:global_step/sec: 782.302\n",
      "INFO:tensorflow:loss = 52.47325, step = 1801 (0.128 sec)\n",
      "INFO:tensorflow:global_step/sec: 833.061\n",
      "INFO:tensorflow:loss = 52.494175, step = 1901 (0.120 sec)\n",
      "INFO:tensorflow:global_step/sec: 827.534\n",
      "INFO:tensorflow:loss = 54.487053, step = 2001 (0.121 sec)\n",
      "INFO:tensorflow:global_step/sec: 813.962\n",
      "INFO:tensorflow:loss = 52.52289, step = 2101 (0.123 sec)\n",
      "INFO:tensorflow:global_step/sec: 824.15\n",
      "INFO:tensorflow:loss = 51.428215, step = 2201 (0.121 sec)\n",
      "INFO:tensorflow:global_step/sec: 842.695\n",
      "INFO:tensorflow:loss = 53.291237, step = 2301 (0.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 828.246\n",
      "INFO:tensorflow:loss = 49.94402, step = 2401 (0.121 sec)\n",
      "INFO:tensorflow:global_step/sec: 755.533\n",
      "INFO:tensorflow:loss = 52.014015, step = 2501 (0.132 sec)\n",
      "INFO:tensorflow:global_step/sec: 751.394\n",
      "INFO:tensorflow:loss = 48.715363, step = 2601 (0.133 sec)\n",
      "INFO:tensorflow:global_step/sec: 735.451\n",
      "INFO:tensorflow:loss = 53.543205, step = 2701 (0.136 sec)\n",
      "INFO:tensorflow:global_step/sec: 761.233\n",
      "INFO:tensorflow:loss = 48.16881, step = 2801 (0.131 sec)\n",
      "INFO:tensorflow:global_step/sec: 789.845\n",
      "INFO:tensorflow:loss = 56.269432, step = 2901 (0.126 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3000 into /var/folders/8t/pw3z265s66x05f3yfpynnr5s3_vhgw/T/tmpupocc4kz/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 59.15136.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-07-17:41:47\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/8t/pw3z265s66x05f3yfpynnr5s3_vhgw/T/tmpupocc4kz/model.ckpt-3000\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-07-17:41:48\n",
      "INFO:tensorflow:Saving dict for global step 3000: accuracy = 0.70125, accuracy_baseline = 0.66625, auc = 0.7114594, auc_precision_recall = 0.6063198, average_loss = 0.5304359, global_step = 3000, label/mean = 0.33375, loss = 53.043594, prediction/mean = 0.3365079\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/8t/pw3z265s66x05f3yfpynnr5s3_vhgw/T/tmpupocc4kz/model.ckpt-3000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def input_fn(features, labels,  batch_size = 100, test = False):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "    if not test:\n",
    "        dataset = dataset.shuffle(1000).repeat()\n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "\n",
    "feature_cols = [tf.feature_column.categorical_column_with_identity(key = 'cluster', num_buckets=n_c),\n",
    "                tf.feature_column.categorical_column_with_identity(key = 'site', num_buckets = n_s)\n",
    "               ]\n",
    "\n",
    "classifier = tf.estimator.LinearClassifier(feature_columns=feature_cols, optimizer=tf.train.AdamOptimizer(learning_rate=0.01))\n",
    "\n",
    "classifier.train(input_fn=lambda: input_fn(x_train, y_train), steps=3000)\n",
    "\n",
    "# Evaluate the model.\n",
    "eval_result = classifier.evaluate(input_fn=lambda: input_fn(x_test, y_test, test=True))\n",
    "\n",
    "y_pred = classifier.predict(input_fn=lambda: input_fn(x_test, y_test, test=True))\n",
    "\n",
    "probs = np.array( [yp['probabilities'][1] for yp in y_pred] )  # predicted probabilities for class[1] \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%RIG =  16.7 %RIG_ideal =  17.07\n"
     ]
    }
   ],
   "source": [
    "\n",
    "RIG = rig(y_test, probs)\n",
    "\n",
    "# true logistic model probabilities \n",
    "true_probs = np.array(list(x_test['prob']))\n",
    "\n",
    "# RIG if we knew the true logistic model\n",
    "ideal_RIG = rig(y_test, true_probs)\n",
    "\n",
    "print(\"%RIG = \", RIG, \"%RIG_ideal = \", ideal_RIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "c_train = x_train['cluster']\n",
    "s_train = x_train['site']\n",
    "n_obs_train = len(c_train)\n",
    "\n",
    "c_test = x_test['cluster']\n",
    "s_test = x_test['site']\n",
    "n_obs_test = len(c_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": "json-false",
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sites: 12\n",
      "Number of clusters: 3\n",
      "Number of observations: 800\n"
     ]
    }
   ],
   "source": [
    "n_s = max(s_train) + 1  # number of sites\n",
    "n_c = max(c_train) + 1  # number of clusters\n",
    "n_obs = len(c_train)  # number of observations\n",
    "\n",
    "print(\"Number of sites: {}\".format(n_s))\n",
    "print(\"Number of clusters: {}\".format(n_c))\n",
    "print(\"Number of observations: {}\".format(n_obs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Model\n",
    "\n",
    "Since our problem is binary classification (convert or not), we use a logistic regression where we model the _log-odds_ as a linear function of predictors.\n",
    "\n",
    "In what follows we let $z$ denote the log-odds, and the actual prediction itself will be $1/(1+e^{-z})$.\n",
    "\n",
    "\n",
    "```\n",
    "z ~ (1|site) + (1|cluster)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Set up placeholders for the data inputs.\n",
    "s_ph = tf.placeholder(tf.int32, [None])\n",
    "c_ph = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# Set up random effects.\n",
    "\n",
    "sigma_s = tf.sqrt(tf.exp(tf.get_variable(\"sigma_s\", [])))\n",
    "sigma_c = tf.sqrt(tf.exp(tf.get_variable(\"sigma_c\", [])))\n",
    "\n",
    "eta_s = Normal(loc=tf.zeros(n_s), scale=sigma_s * tf.ones(n_s))\n",
    "eta_c = Normal(loc=tf.zeros(n_c), scale=sigma_c * tf.ones(n_c))\n",
    "\n",
    "yhat = (tf.gather(eta_s, s_ph) + # pick the entry from eta_s using site-index fed into placeholder s_ph \n",
    "        tf.gather(eta_c, c_ph))  # same thing with cluster-index fed into placeholder c_ph\n",
    "\n",
    "yhat_s = tf.gather(eta_s, s_ph) # site_only model\n",
    "\n",
    "# y_logit = Normal(loc=yhat, scale=tf.ones(n_obs))\n",
    "\n",
    "y = Bernoulli(logits = yhat)\n",
    "y_s = Bernoulli(logits = yhat_s)  # site_only model\n",
    "\n",
    "\n",
    "# y = tf.sigmoid(y_logit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Inference\n",
    "\n",
    "Given data, we aim to infer the model's fixed and random effects.\n",
    "In this analysis, we use variational inference with the\n",
    "$\\text{KL}(q\\|p)$ divergence measure. We specify fully factorized\n",
    "normal approximations for the random effects and pass in all training\n",
    "data for inference. Under the algorithm, the fixed effects will be\n",
    "estimated under a variational EM scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pchalasani/miniconda/envs/tfbrain/lib/python3.6/site-packages/edward/util/random_variables.py:52: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  not np.issubdtype(value.dtype, np.float) and \\\n",
      "/Users/pchalasani/miniconda/envs/tfbrain/lib/python3.6/site-packages/edward/util/random_variables.py:53: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  not np.issubdtype(value.dtype, np.int) and \\\n"
     ]
    }
   ],
   "source": [
    "q_eta_s = Normal(\n",
    "    loc=tf.get_variable(\"q_eta_s/loc\", [n_s]),\n",
    "    scale=tf.nn.softplus(tf.get_variable(\"q_eta_s/scale\", [n_s])))\n",
    "q_eta_c = Normal(\n",
    "    loc=tf.get_variable(\"q_eta_c/loc\", [n_c]),\n",
    "    scale=tf.nn.softplus(tf.get_variable(\"q_eta_c/scale\", [n_c])))\n",
    "\n",
    "latent_vars = {\n",
    "    eta_s: q_eta_s,\n",
    "    eta_c: q_eta_c}\n",
    "\n",
    "data = {\n",
    "    y: y_train,\n",
    "    s_ph: s_train,\n",
    "    c_ph: c_train}\n",
    "\n",
    "data_s = {\n",
    "    y_s: y_train,\n",
    "    s_ph: s_train}\n",
    "\n",
    "\n",
    "inference = ed.KLqp(latent_vars, data)\n",
    "\n",
    "inference_s = ed.KLqp({eta_s: q_eta_s}, data_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criticism\n",
    "\n",
    "We will evaluate the inferred distributions by computing logits from the means of the inferred posterior distributions of the latent vars. From the logits we can compute the log-loss relative to the observed 0/1 labels, and compute the RIG from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "yhat_test = ed.copy(yhat, {\n",
    "    eta_s: q_eta_s.mean(),\n",
    "    eta_c: q_eta_c.mean()})\n",
    "\n",
    "yhat_test_s = ed.copy(yhat_s, {\n",
    "    eta_s: q_eta_s.mean()})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": "json-false",
    "ein.tags": [
     "worksheet-0"
    ],
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1/10000 [  0%]                                ETA: 9737s | Loss: 580.906rig= -14.19\n",
      " 2000/10000 [ 20%] ██████                         ETA: 12s | Loss: 458.938  rig= 16.34\n",
      " 4000/10000 [ 40%] ████████████                   ETA: 7s | Loss: 454.474 rig= 16.38\n",
      " 6000/10000 [ 60%] ██████████████████             ETA: 4s | Loss: 452.398rig= 16.38\n",
      " 8000/10000 [ 80%] ████████████████████████       ETA: 2s | Loss: 451.070rig= 16.39\n",
      "10000/10000 [100%] ██████████████████████████████ Elapsed: 11s | Loss: 453.829\n",
      "rig= 16.39\n"
     ]
    }
   ],
   "source": [
    "inference.initialize(n_print=2000, n_iter=10000)\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "\n",
    "for _ in range(inference.n_iter):\n",
    "  # Update and print progress of algorithm.\n",
    "  info_dict = inference.update()\n",
    "\n",
    "  inference.print_progress(info_dict)\n",
    "\n",
    "  t = info_dict['t']\n",
    "  if t == 1 or t % inference.n_print == 0:\n",
    "    # Make predictions on test data.\n",
    "    yhat_vals = yhat_test.eval(feed_dict={\n",
    "        s_ph: s_test,\n",
    "        c_ph: c_test})\n",
    "\n",
    "    probs = logistic.cdf(yhat_vals)\n",
    "    rg  = rig(y_test, probs)\n",
    "    \n",
    "    print('rig=', rg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1/10000 [  0%]                                ETA: 6797s | Loss: 45143.887rig= -6.08\n",
      " 2000/10000 [ 20%] ██████                         ETA: 18s | Loss: 27015.959  rig= 16.06\n",
      " 4000/10000 [ 40%] ████████████                   ETA: 12s | Loss: 27015.629rig= 16.07\n",
      " 6000/10000 [ 60%] ██████████████████             ETA: 7s | Loss: 27010.602 rig= 16.07\n",
      " 8000/10000 [ 80%] ████████████████████████       ETA: 3s | Loss: 27013.801rig= 16.07\n",
      "10000/10000 [100%] ██████████████████████████████ Elapsed: 19s | Loss: 27010.979\n",
      "rig= 16.07\n"
     ]
    }
   ],
   "source": [
    "inference_s.initialize(n_print=2000, n_iter=10000)\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "\n",
    "for _ in range(inference_s.n_iter):\n",
    "  # Update and print progress of algorithm.\n",
    "  info_dict = inference_s.update()\n",
    "\n",
    "  inference_s.print_progress(info_dict)\n",
    "\n",
    "  t = info_dict['t']\n",
    "  if t == 1 or t % inference.n_print == 0:\n",
    "    # Make predictions on test data.\n",
    "    yhat_vals = yhat_test_s.eval(feed_dict={\n",
    "        s_ph: s_test})\n",
    "\n",
    "\n",
    "    probs = logistic.cdf(yhat_vals)\n",
    "    rg  = rig(y_test, probs)\n",
    "    \n",
    "    print('rig=', rg)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "name": "linear_mixed_effects_models.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
