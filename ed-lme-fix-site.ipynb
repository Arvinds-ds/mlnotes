{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Linear Mixed Effects Models -- modified Edward tutorial for grouped model\n",
    "\n",
    "### This version uses fixed effects for sites, random effects for clusters\n",
    "\n",
    "\n",
    "With linear mixed effects models, we wish to model a linear\n",
    "relationship for data points with inputs of varying type, categorized\n",
    "into subgroups, and associated to a real-valued output.\n",
    "\n",
    "We demonstrate with an example in Edward. A webpage version is available \n",
    "[here](http://edwardlib.org/tutorials/linear-mixed-effects-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import edward as ed\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from edward.models import Normal, BernoulliWithSigmoidProbs, Bernoulli\n",
    "from observations import insteval\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import logistic\n",
    "\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "ed.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Data: Clusters and site parameters\n",
    "\n",
    "We will define 3 clusters, each with some number of member sites.\n",
    "\n",
    "The data-generating model follows a very simple premise:\n",
    "> Within each cluster, the sites will have a \"default\" or \"baseline\" RR (response-rate), and sites within a cluster will exhibit variation around this cluster-default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# site, cluster occurrence probabilities, to generate the dataset\n",
    "# as well as their \"true\" logit-weights \n",
    "\n",
    "clusters = {0: dict(prob = 0.2, w = -0.5,   # cluster weight in logistic\n",
    "                    sites = np.arange(6),     \n",
    "                    site_probs = [0.1, 0.1, 0.1, 0.1, 0.1, 0.5],\n",
    "                    w_s =        [0.0, 0.0, 0.0, 5.0, 0.0, 0.0]),  # weights of sites in logistic\n",
    "            1: dict(prob = 0.5, w = -1.0,  \n",
    "                    sites = 6 + np.arange(3), \n",
    "                    site_probs = [0.1, 0.3, 0.6],\n",
    "                    w_s        = [0.0, 0.0, 0.0]),                    \n",
    "            2: dict(prob = 0.3, w = -0.2,  \n",
    "                    sites = 9 + np.arange(3), \n",
    "                    site_probs = [0.3, 0.3, 0.4] ,\n",
    "                    w_s =        [0.0, -4, 0.0 ]\n",
    "                   )}\n",
    "\n",
    "\n",
    "# num clusters\n",
    "n_c = len(clusters)\n",
    "\n",
    "# num sites\n",
    "n_s = sum(list( map(len, list( map( lambda d: d['sites'], clusters.values()))))) # num sites\n",
    "\n",
    "# site to cluster map\n",
    "s2c = dict( [ (s,c) for c in range(n_c) for s in clusters[c]['sites']] )\n",
    "\n",
    "# prob of each cluster occurring\n",
    "p_c = [c['prob'] for c in clusters.values()]\n",
    "\n",
    "# prob of site occurrence, within a cluster\n",
    "\n",
    "p_s = ( [  dict( zip (d['sites'], d['site_probs']))  for d in clusters.values() ])\n",
    "\n",
    "# \"true\" weights of clusters in logit model\n",
    "w_c = np.array( [d['w'] for d in clusters.values()] )\n",
    "\n",
    "# \"true\" weights of sites in logit model\n",
    "w_s = np.concatenate( [d['w_s'] for d in clusters.values() ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True weights of logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logit for a site: sum of site-weight and its cluster-weight\n",
    "def logit_site(site):\n",
    "    return w_c[s2c[site]] + w_s[site]\n",
    "\n",
    "# site response_rate\n",
    "def rr_s(site):\n",
    "    return logistic.cdf(logit_site(site))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Site response-rates\n",
    "Note how this matches what we wanted to model, i.e. in each cluster sites have a certain \"baseline\" response rate (RR), and some have much higher or much lower RR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3775,\n",
       " 0.3775,\n",
       " 0.3775,\n",
       " 0.989,\n",
       " 0.3775,\n",
       " 0.3775,\n",
       " 0.2689,\n",
       " 0.2689,\n",
       " 0.2689,\n",
       " 0.4502,\n",
       " 0.0148,\n",
       " 0.4502]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.round(rr_s(s),4) for s in range(n_s) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data gen \n",
    "We generate N rows with `[cluster_id, site_id, abel]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_label(site):\n",
    "    p = rr_s(site)\n",
    "    return (np.random.uniform() < p)*1\n",
    "\n",
    "\n",
    "def gen_row(cluster):\n",
    "    site2prob = p_s[cluster]\n",
    "    site_ids = list( site2prob.keys())\n",
    "    probs = list(site2prob.values())\n",
    "    site = site_ids[ np.random.choice(len(probs), 1, probs ) [0] ]\n",
    "    return np.array( [ cluster, site, gen_label(site)] )\n",
    "\n",
    "def gen_data(N=100):\n",
    "    clusters = np.random.choice(n_c, N, list(p_c))\n",
    "    return np.array(list(map(gen_row, clusters)))\n",
    "\n",
    "train = gen_data(N=1000)\n",
    "test = gen_data(N=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2, 11,  1],\n",
       "       [ 0,  4,  0],\n",
       "       [ 2, 11,  0],\n",
       "       [ 2, 11,  0],\n",
       "       [ 0,  5,  0],\n",
       "       [ 0,  5,  1],\n",
       "       [ 2, 11,  0],\n",
       "       [ 1,  7,  0],\n",
       "       [ 2, 10,  0],\n",
       "       [ 2,  9,  1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "c_train = train[:,0]\n",
    "s_train = train[:,1]\n",
    "y_train = train[:,2]\n",
    "n_obs_train = train.shape[0]\n",
    "\n",
    "c_test = test[:,0]\n",
    "s_test = test[:,1]\n",
    "y_test = np.array(test[:,2]).astype(np.int32)\n",
    "n_obs_test = test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "json-false",
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sites: 12\n",
      "Number of clusters: 3\n",
      "Number of observations: 1000\n"
     ]
    }
   ],
   "source": [
    "n_s = max(s_train) + 1  # number of sites\n",
    "n_c = max(c_train) + 1  # number of clusters\n",
    "n_obs = train.shape[0]  # number of observations\n",
    "\n",
    "print(\"Number of sites: {}\".format(n_s))\n",
    "print(\"Number of clusters: {}\".format(n_c))\n",
    "print(\"Number of observations: {}\".format(n_obs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Model\n",
    "\n",
    "Since our problem is binary classification (convert or not), we use a logistic regression where we model the _log-odds_ as a linear function of predictors.\n",
    "\n",
    "In what follows we let $z$ denote the log-odds, and the actual prediction itself will be $1/(1+e^{-z})$.\n",
    "\n",
    "\n",
    "```\n",
    "z ~ (1|site) + cluster\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Set up placeholders for the data inputs.\n",
    "s_ph = tf.placeholder(tf.int32, [None])\n",
    "c_ph = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# Set up random effects.\n",
    "\n",
    "sigma_c = tf.sqrt(tf.exp(tf.get_variable(\"sigma_c\", [])))  # random effect for clusters\n",
    "beta_s = tf.get_variable(\"beta_s\", [n_s])  # fixed effect for site\n",
    "\n",
    "eta_c = Normal(loc=tf.zeros(n_c), scale=sigma_c * tf.ones(n_c))\n",
    "\n",
    "\n",
    "yhat = (tf.gather(eta_c, c_ph) + # pick the entry from eta_s using site-index fed into placeholder s_ph \n",
    "        tf.gather(beta_s, s_ph))  # same thing with cluster-index fed into placeholder c_ph\n",
    "\n",
    "# y_logit = Normal(loc=yhat, scale=tf.ones(n_obs))\n",
    "\n",
    "y = Bernoulli(logits = yhat)\n",
    "\n",
    "\n",
    "\n",
    "# y = tf.sigmoid(y_logit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Inference\n",
    "\n",
    "Given data, we aim to infer the model's fixed and random effects.\n",
    "In this analysis, we use variational inference with the\n",
    "$\\text{KL}(q\\|p)$ divergence measure. We specify fully factorized\n",
    "normal approximations for the random effects and pass in all training\n",
    "data for inference. Under the algorithm, the fixed effects will be\n",
    "estimated under a variational EM scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pchalasani/miniconda/envs/tfbrain/lib/python3.6/site-packages/edward/util/random_variables.py:52: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  not np.issubdtype(value.dtype, np.float) and \\\n",
      "/Users/pchalasani/miniconda/envs/tfbrain/lib/python3.6/site-packages/edward/util/random_variables.py:53: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  not np.issubdtype(value.dtype, np.int) and \\\n"
     ]
    }
   ],
   "source": [
    "q_eta_c = Normal(\n",
    "    loc=tf.get_variable(\"q_eta_c/loc\", [n_c]),\n",
    "    scale=tf.nn.softplus(tf.get_variable(\"q_eta_c/scale\", [n_c])))\n",
    "\n",
    "latent_vars = {eta_c: q_eta_c}\n",
    "\n",
    "data = {\n",
    "    y: y_train,\n",
    "    s_ph: s_train,\n",
    "    c_ph: c_train}\n",
    "\n",
    "\n",
    "inference = ed.KLqp(latent_vars, data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criticism\n",
    "\n",
    "We will evaluate the inferred distributions by computing logits from the means of the inferred posterior distributions of the latent vars. From the logits we can compute the log-loss relative to the observed 0/1 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": true,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "yhat_test = ed.copy(yhat, {eta_c: q_eta_c.mean() })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(labels, probs):\n",
    "    return -np.mean(labels * np.log(probs) + (1-labels)*np.log(1-probs))\n",
    "\n",
    "def rig(labels, probs):\n",
    "    p = np.mean(labels)\n",
    "    ent = -p*np.log(p) - (1-p)*np.log(1-p)\n",
    "    loss = log_loss(labels, probs)\n",
    "    return np.round(100*(ent - loss)/ent, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": "json-false",
    "ein.tags": [
     "worksheet-0"
    ],
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1/20000 [  0%]                                ETA: 16070s | Loss: 740.791rig= -12.66\n",
      " 2000/20000 [ 10%] ███                            ETA: 23s | Loss: 549.892   rig= 11.66\n",
      " 4000/20000 [ 20%] ██████                         ETA: 17s | Loss: 549.735rig= 11.53\n",
      " 6000/20000 [ 30%] █████████                      ETA: 14s | Loss: 549.696rig= 11.51\n",
      " 8000/20000 [ 40%] ████████████                   ETA: 11s | Loss: 549.790rig= 11.51\n",
      "10000/20000 [ 50%] ███████████████                ETA: 9s | Loss: 549.711 rig= 11.5\n",
      "12000/20000 [ 60%] ██████████████████             ETA: 7s | Loss: 549.699rig= 11.5\n",
      "14000/20000 [ 70%] █████████████████████          ETA: 5s | Loss: 549.772rig= 11.5\n",
      "16000/20000 [ 80%] ████████████████████████       ETA: 3s | Loss: 549.807rig= 11.5\n",
      "18000/20000 [ 90%] ███████████████████████████    ETA: 1s | Loss: 549.725rig= 11.5\n",
      "20000/20000 [100%] ██████████████████████████████ Elapsed: 18s | Loss: 549.697\n",
      "rig= 11.5\n"
     ]
    }
   ],
   "source": [
    "inference.initialize(n_print=2000, n_iter=20000)\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "\n",
    "for _ in range(inference.n_iter):\n",
    "  # Update and print progress of algorithm.\n",
    "  info_dict = inference.update()\n",
    "\n",
    "  inference.print_progress(info_dict)\n",
    "\n",
    "  t = info_dict['t']\n",
    "  if t == 1 or t % inference.n_print == 0:\n",
    "    # Make predictions on test data.\n",
    "    yhat_vals = yhat_test.eval(feed_dict={\n",
    "        s_ph: s_test,\n",
    "        c_ph: c_test})\n",
    "\n",
    "    probs = logistic.cdf(yhat_vals)\n",
    "    rg  = rig(y_test, probs)\n",
    "    \n",
    "    print('rig=', rg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1/10000 [  0%]                                ETA: 7054s | Loss: 675.057rig= -10.78\n",
      " 2000/10000 [ 20%] ██████                         ETA: 9s | Loss: 575.155   rig= 12.61\n",
      " 4000/10000 [ 40%] ████████████                   ETA: 6s | Loss: 578.508rig= 12.48\n",
      " 6000/10000 [ 60%] ██████████████████             ETA: 3s | Loss: 579.477rig= 12.51\n",
      " 8000/10000 [ 80%] ████████████████████████       ETA: 1s | Loss: 575.550rig= 12.52\n",
      "10000/10000 [100%] ██████████████████████████████ Elapsed: 8s | Loss: 577.369\n",
      "rig= 12.52\n"
     ]
    }
   ],
   "source": [
    "inference_s.initialize(n_print=2000, n_iter=10000)\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "\n",
    "for _ in range(inference_s.n_iter):\n",
    "  # Update and print progress of algorithm.\n",
    "  info_dict = inference_s.update()\n",
    "\n",
    "  inference_s.print_progress(info_dict)\n",
    "\n",
    "  t = info_dict['t']\n",
    "  if t == 1 or t % inference.n_print == 0:\n",
    "    # Make predictions on test data.\n",
    "    yhat_vals = yhat_test_s.eval(feed_dict={\n",
    "        s_ph: s_test})\n",
    "\n",
    "\n",
    "    probs = logistic.cdf(yhat_vals)\n",
    "    rg  = rig(y_test, probs)\n",
    "    \n",
    "    print('rig=', rg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with MLE logistic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "name": "linear_mixed_effects_models.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
